---
layout: distill 
title: Foundations of Probability and its links to AI
description: 
tags: 
- ML mathematics
giscus_comments: false 
date: 2022-05-28
featured: false

---
<h1 style="text-align: center; color: blue !important;">Foundations of Probability and its links to AI</h1>


##### Haydar jawad, May 2022 #  Foundations of Probability

## What are Axioms?

Axioms are basic rules or principles that we accept as true without needing proof. They are like the building blocks or foundations of a theory.

## What is Probability?

Probability is a way of measuring how likely something is to happen. It’s like saying how sure you are that an event will occur.

## The Three Axioms of Probability

1. **Non-negativity (Axiom 1)**

    This means that the probability of any event is always a non-negative number (zero or positive). You can't have a negative chance of something happening.

    **Example:**
    If you have a set of images and you're classifying them as either cat images or not, the probability of an image being a cat ($$ P(\text{cat}) $$) is always a number from 0 to 1.

2. **Normalisation (Axiom 2)**

    The total probability of all possible outcomes in the sample space is 1. This means that something must happen; the sum of probabilities of all events equals 1.

    **Example:**
    If you have a machine learning model that predicts whether an image is a cat or not, the probabilities of being a cat ($$ P(\text{cat}) $$) and not being a cat ($$ P(\text{not cat}) $$) must add up to 1.

3. **Additivity (Axiom 3)**

    If two events cannot happen at the same time (they are mutually exclusive), the probability that one or the other happens is the sum of their individual probabilities.

    **Example:**
    If you have an image classification model that predicts either a cat or a dog (and an image cannot be both at the same time), the probability of the image being a cat or a dog is the sum of the probability of it being a cat and the probability of it being a dog.

## Applying the Axioms in AI

Let’s see how these axioms apply in AI, particularly in a scenario where we use a machine learning model to classify images as either containing cats or dogs.

**Example: Image Classification in AI**

- **Non-negativity (Axiom 1):**
  Your AI model assigns probabilities to images. For any image, the probability $$ P(\text{cat}) $$ must be between 0 and 1.

  **Example:**
  An image has a 0.8 probability of being a cat. This means the model is 80% sure the image is a cat. You can't have a -0.5 or 1.5 probability.

- **Normalisation (Axiom 2):**
  The model assigns probabilities to all possible categories (cat or dog). The total probability must add up to 1.

  **Example:**
  If the model gives $$ P(\text{cat}) = 0.7 $$, then $$ P(\text{dog}) = 0.3 $$ because $$ 0.7 + 0.3 = 1 $$.

- **Additivity (Axiom 3):**
  For mutually exclusive events (an image can't be both a cat and a dog at the same time), the probability of either event happening is the sum of their probabilities.

  **Example:**
  If the model predicts that an image has a 0.6 probability of being a cat and a 0.4 probability of being a dog, the combined probability of the image being either a cat or a dog is:
  $$ P(\text{cat or dog}) = P(\text{cat}) + P(\text{dog}) = 0.6 + 0.4 = 1 $$.

## Recap with Formulas

- **Non-negativity:** $$ P(A) \geq 0 $$ for any event $$ A $$.
- **Normalisation:** $$ P(S) = 1 $$ where $$ S $$ is the sample space.
- **Additivity:** If $$ A $$ and $$ B $$ are mutually exclusive, $$ P(A \cup B) = P(A) + P(B) $$.

## Visualising with an Example

Imagine you have 10 images, and your AI model classifies each image. Here’s how the axioms work:

- **Non-negativity:** The probability of any image being a cat or dog is always between 0 and 1.
- **Normalisation:** For each image, the probabilities of all possible outcomes (cat or dog) add up to 1.
- **Additivity:** If an image can't be both a cat and a dog, the probability of it being either a cat or a dog is the sum of the two individual probabilities.

By understanding and applying these axioms, AI systems can make reliable and consistent predictions based on probabilities.
